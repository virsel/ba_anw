# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json

# Learn more about building a configuration: https://promptfoo.dev/docs/configuration/guide

description: "My eval"

prompts:
  - "Write a tweet about {{topic}}"
  - "Write a concise, funny tweet about {{topic}}"
  

providers:
  - id: openai:gpt-4o-mini
    config:
      temperature: 0
      max_tokens: 128
      apiKey: file://api_key.txt
  - id: openai:gpt-4o
    config:
      temperature: 0
      max_tokens: 128
      apiKey: file://api_key.txt
  - id: openai:chat:llama-3.2-1b-instruct  # Dein Modellbezeichner für den lokalen Server
    config:
      apiBaseUrl: http://localhost:1234/v1  # URL des lokalen Servers
      apiKey: lm-studio  # Dummy-API-Schlüssel für den lokalen Server


defaultTest:
  options:
    provider:
        id: openai:gpt-4o
        config:
          apiKey: file://api_key.txt

#  options:
#    provider:
#        id: openai:chat:llama-3.2-3b-instruct
#        config:
#         apiBaseUrl: http://localhost:1234/v1
#         apiKey: lm-studio
         
tests:
  - vars:
      topic: bananas

  - vars:
      topic: avocado toast
    assert:
      # For more information on assertions, see https://promptfoo.dev/docs/configuration/expected-outputs

      # Make sure output contains the word "avocado"
      - type: icontains
        value: avocado

      # Prefer shorter outputs
      - type: javascript
        value: 1 / (output.length + 1)

  - vars:
      topic: new york city
    assert:
#      # For more information on model-graded evals, see https://promptfoo.dev/docs/configuration/expected-outputs/model-graded
      - type: llm-rubric
        value: ensure that the output is funny
        apiKey: file://api_key.txt
