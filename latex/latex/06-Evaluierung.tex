% Henning + Alex (1000)

\chapter{Evaluierung}
\label{cha:eval}

Dieses Kapitel beschäftigt sich, \acro{CRISP-DM} folgend, mit der Evaluierung des zuvor erarbeiteten und in \autoref{cha:Modellierung} beschriebenen Modells. Dabei ist im Kern die wichtigste Komponente der Prompt, da dieser die relevanteste Stellschraube ist um das Ergebnis zu beeinflussen. Zwar wurden auch mit verschiedene LLMs interagiert, dies deinte aber mehr der generlisierung des Ansatzes, weniger der Vergleich der verschienenden \acro{LLM}-Varianten.

Während interaktiv noch gut festgestellt werden kann, ob die Antwortqualität eines \acro{LLM} zunimmt oder abnimmt ist dies bei der automatisierten, programmatischen Nutzung schwieriger. Im vorliegenden Fall erfolgt die Nutzung des \acro{LLM}s wie unter \autoref{sec:prompte} beschrieben mittels \acro{API}. Die Idee war daher, auch die Ergebnisse programmatisch zu überprüfen.

Kernidee: Erkläre warum evaluiert wird

Wie evaluiert wird

Welche Technik und Literatur zum evaluieren verwendet wird.

Wie die Ergebnisse aussehen, wo "gut" klappte, was noch besser sein könnte. Ggf. Was richtung "Learning/Conclusion". Einhaltung CRISP-DM zeigen und aufs iterative referenzieren und Rücksprung auf die ersten Kapitel machen.

Hier kommt eine Übersicht des Evaluierung Kapitels hin.

\section{Eval1}
\label{sec:abschnitt3.1}

\section{Eval2}

\section{Eval3}